{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# gsw oceanic toolbox: http://www.teos-10.org/pubs/Getting_Started.pdf\n",
    "import gsw\n",
    "from scipy.io import loadmat\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from processing_func import check_coords, calc_N2_kappa, calc_hab, arctic_calchab, calc_N2_kappa_sorted, calc_sic\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_mix = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/arctic_mix.nc\"\n",
    "alberto_nc = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/alberto_ds.nc\"\n",
    "global_pkl = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/input_microstructure.pkl\"\n",
    "global_nc = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/L2_2D_snapshot_iy150_model_input.nc\"\n",
    "mosaic_nc = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/mosaic_ds.nc\"\n",
    "nice_nc = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/nice_ds.nc\"\n",
    "HM_nc = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/HM_ds.nc\"\n",
    "NN_nc = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/L2_2D_snapshot_iy150_model_input.nc\"\n",
    "\n",
    "arctic_ds = xr.open_dataset(arctic_mix)\n",
    "alberto_ds = xr.open_dataset(alberto_nc)\n",
    "global_ds = pd.read_pickle(global_pkl)\n",
    "global_nn = xr.open_dataset(global_nc)\n",
    "mosaic_ds = xr.open_dataset(mosaic_nc)\n",
    "nice_ds = xr.open_dataset(nice_nc)\n",
    "HM_ds = xr.open_dataset(HM_nc)\n",
    "NN_ds = xr.open_dataset(NN_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the 'time' coordinate as a variable in the copy\n",
    "nice_ds = nice_ds.rename({\"time\": \"time_dim\"})\n",
    "nice_ds[\"time\"] = nice_ds[\"time_dim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_ds = arctic_ds.reset_coords([\"latitude\", \"longitude\", \"time\"])\n",
    "mosaic_ds = mosaic_ds.reset_coords([\"latitude\", \"longitude\", \"time\"])\n",
    "alberto_ds = alberto_ds.reset_coords([\"latitude\", \"longitude\", \"time\"])\n",
    "nice_ds = nice_ds.reset_coords([\"latitude\", \"longitude\", \"time\"])\n",
    "HM_ds = HM_ds.reset_coords([\"latitude\", \"longitude\", \"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bathymetry dataset\n",
    "GEBCO_ds = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/gebco_2022_n80.0_s63.0_w-170.0_e-130.0.nc\"\n",
    "bathy_ds = xr.open_dataset(GEBCO_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sea ice fraction data\n",
    "SI_HadISST = \"/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/SI-area/HadISST_ice.nc\"\n",
    "Hadi_SI = xr.open_dataset(SI_HadISST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add all features and combine all datasets into one dataframe\n",
    "The features and the plots are explained below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datetime(dataset):\n",
    "    if type(dataset.time.values[0]) is not np.datetime64:\n",
    "        dataset[\"time\"] = xr.apply_ufunc(pd.to_datetime, dataset[\"time\"], vectorize=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_ds = convert_datetime(arctic_ds)\n",
    "mosaic_ds = convert_datetime(mosaic_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_values = alberto_ds[\"time\"].values\n",
    "time_index = np.where(alberto_ds[\"time\"].values == time_values[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m alberto_kappa \u001b[39m=\u001b[39m calc_N2_kappa_sorted(alberto_ds)\n\u001b[1;32m     12\u001b[0m nice_kappa \u001b[39m=\u001b[39m calc_sic(nice_kappa, Hadi_SI)\n\u001b[0;32m---> 13\u001b[0m alberto_kappa \u001b[39m=\u001b[39m calc_sic(alberto_kappa, Hadi_SI)\n\u001b[1;32m     14\u001b[0m HM_kappa \u001b[39m=\u001b[39m calc_sic(HM_kappa, Hadi_SI)\n\u001b[1;32m     15\u001b[0m arctic_kappa \u001b[39m=\u001b[39m calc_sic(arctic_kappa, Hadi_SI)\n",
      "File \u001b[0;32m~/Documents/AI4ER/Mres/ArcticTurbulence/data_exploration/processing_func.py:348\u001b[0m, in \u001b[0;36mcalc_sic\u001b[0;34m(dataset, Hadi_SI)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m# Loop over each profile in dataset\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mfor\u001b[39;00m i, profile \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset[\u001b[39m\"\u001b[39m\u001b[39mprofile\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    347\u001b[0m     \u001b[39m# Get the corresponding indices of time, latitude, and longitude\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     time_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mwhere(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues \u001b[39m==\u001b[39;49m time_values[i])[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    349\u001b[0m     lat_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(dataset[\u001b[39m\"\u001b[39m\u001b[39mnearest_lat\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues \u001b[39m==\u001b[39m\n\u001b[1;32m    350\u001b[0m                          latitude_values[i])[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    351\u001b[0m     lon_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(dataset[\u001b[39m\"\u001b[39m\u001b[39mnearest_lon\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues \u001b[39m==\u001b[39m\n\u001b[1;32m    352\u001b[0m                          longitude_values[i])[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "HM_kappa = HM_ds.squeeze()\n",
    "arctic_ds = arctic_ds.squeeze()\n",
    "arctic_ds = arctic_ds.transpose('depth', 'profile')\n",
    "HM_kappa = HM_kappa.transpose('depth', 'profile')\n",
    "\n",
    "nice_kappa = calc_N2_kappa_sorted(nice_ds)\n",
    "mosaic_kappa = calc_N2_kappa_sorted(mosaic_ds)\n",
    "HM_kappa = calc_N2_kappa_sorted(HM_kappa)\n",
    "arctic_kappa = calc_N2_kappa_sorted(arctic_ds)\n",
    "alberto_kappa = calc_N2_kappa_sorted(alberto_ds)\n",
    "\n",
    "nice_kappa = calc_sic(nice_kappa, Hadi_SI)\n",
    "alberto_kappa = calc_sic(alberto_kappa, Hadi_SI)\n",
    "HM_kappa = calc_sic(HM_kappa, Hadi_SI)\n",
    "arctic_kappa = calc_sic(arctic_kappa, Hadi_SI)\n",
    "alberto_kappa = calc_sic(alberto_kappa, Hadi_SI)\n",
    "\n",
    "arctic_final = arctic_calchab(arctic_kappa, bathy_ds)\n",
    "nice_final = calc_hab(nice_kappa, bathy_ds)\n",
    "mosaic_final = calc_hab(mosaic_kappa, bathy_ds)\n",
    "HM_final = calc_hab(HM_kappa, bathy_ds)\n",
    "alberto_final = calc_hab(alberto_kappa, bathy_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add variables of the cruise name\n",
    "arctic_final[\"cruise\"] = \"ArcticMix\"\n",
    "nice_final[\"cruise\"] = \"NICE\"\n",
    "mosaic_final[\"cruise\"] = \"Mosaic\"\n",
    "alberto_final[\"cruise\"] = \"Alberto\"\n",
    "HM_final[\"cruise\"] = \"HM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_fin = nice_final\n",
    "nice_fin = nice_fin.rename({\"time\": \"time-coord\"})\n",
    "nice_fin[\"time\"] = nice_fin[\"time-coord\"]\n",
    "nice_fin[\"time\"] = xr.DataArray(nice_fin[\"time\"].values, dims='profile')\n",
    "nice_selected = nice_fin.drop_vars(['time-coord', 'LATITUDE', \"LONGITUDE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select variables from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_variables(data, variables):\n",
    "    \"\"\"\n",
    "    Select specific variables from the given dataset and reduce dimensions and coordinates to depth and profile.\n",
    "    Args:\n",
    "        data (xarray.Dataset): Input dataset containing variables.\n",
    "        variables (list): List of variable names to select.\n",
    "    Returns:\n",
    "        xarray.Dataset: Dataset with selected variables and reduced dimensions/coordinates.\n",
    "    \"\"\"\n",
    "    # Select the desired variables from the dataset\n",
    "    selected_data = data[variables]\n",
    "    # Reduce dimensions and coordinates to depth and profile\n",
    "    selected_data = selected_data.squeeze().reset_coords(drop=True)\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns = [\"depth\", \"profile\", \"latitude\", \"longitude\", \"P\", \"S\", \"T\", \"Tu\", \"kappa\", \"log_N2\", \"log_kappa\", \"log_eps\", \"dTdz\", \"dSdz\", \"eps\", \"cruise\", \"hab\", \"Tu_label\", \"time\"]\n",
    "selected_columns = [\"depth\", \"profile\", \"latitude\", \"longitude\", \"cruise\", \"S\", \"T\", \"eps\", \"log_N2\", \"dTdz\", \"dSdz\", \"hab\", \"Tu\", \"Tu_label\", \"time\"]\n",
    "\n",
    "arctic_f = select_variables(arctic_final, selected_columns)\n",
    "alberto_f = select_variables(alberto_final, selected_columns)\n",
    "mosaic_f = select_variables(mosaic_final, selected_columns)\n",
    "HM_f = select_variables(HM_final, selected_columns)\n",
    "nice_f = select_variables(nice_selected, selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_df = arctic_f.to_dataframe().reset_index()\n",
    "HM_df = HM_f.to_dataframe().reset_index()\n",
    "mosaic_df = mosaic_f.to_dataframe().reset_index()\n",
    "alberto_df = alberto_f.to_dataframe().reset_index()\n",
    "nice_df = nice_f.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627907"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([arctic_df, HM_df, mosaic_df, alberto_df, nice_df])\n",
    "combined_nona = combined_df.copy()\n",
    "combined_nona = combined_nona.dropna()\n",
    "len(combined_nona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2101764 without\n",
    "1307594 with P, S, T\n",
    "1287803 with P, S, T, eps ## baseline\n",
    "876875 with P, S, T, eps, logN2\n",
    "876875 with P, S, T, eps, logN2, dTdz, dS,dz # all still aboard\n",
    "867841 with P, S, T, eps, logN2, dTdz, dS,dz, hab\n",
    "867841 with P, S, T, eps, logN2, dTdz, dS,dz, hab, Tu & Tu_label\n",
    "862023 with P, S, T, eps, logN2, dTdz, dS,dz, hab, Tu & Tu_label and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ArcticMix', 'HM', 'Mosaic', 'Alberto', 'NICE'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_nona.cruise.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_nona.to_pickle('/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/ml_ready/2905all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise this non na dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('/Users/Lisanne/Documents/AI4ER/Mres/ArcticTurbulence/data/processed_data/ml_ready/2605all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.NorthPolarStereo())\n",
    "\n",
    "# Add land, ocean, and borders\n",
    "ax.add_feature(cfeature.OCEAN.with_scale('50m'))\n",
    "ax.add_feature(cfeature.LAND.with_scale('50m'))\n",
    "\n",
    "# Convert non-numeric cruise values to category codes\n",
    "full_df[\"cruise_codes\"] = full_df[\"cruise\"].astype(\"category\").cat.codes\n",
    "\n",
    "ax.scatter(full_df[\"longitude\"], full_df[\"latitude\"], c=full_df[\"cruise_codes\"],\n",
    "           transform=ccrs.PlateCarree(), s=5)\n",
    "\n",
    "#ax.scatter(full_df[\"longitude\"], full_df[\"latitude\"], c = full_df[\"cruise\"], transform=ccrs.PlateCarree(), s=5)\n",
    "\n",
    "# Add grid lines\n",
    "gl = ax.gridlines(draw_labels=True, linestyle='--')\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_right = False\n",
    "gl.bottom_labels = True  # Show x-axis labels at the bottom\n",
    "gl.left_labels = True  # Show y-axis labels on the left\n",
    "\n",
    "ax.set_extent([-180, 180, 65, 90], crs=ccrs.PlateCarree())\n",
    "ax.set_title(\"Round Map with Grid Lines and Plot Points\")\n",
    "ax.legend(loc=\"center left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.NorthPolarStereo())\n",
    "\n",
    "# Add land, ocean, and borders\n",
    "ax.add_feature(cfeature.OCEAN.with_scale('50m'))\n",
    "ax.add_feature(cfeature.LAND.with_scale('50m'))\n",
    "\n",
    "# Convert non-numeric cruise values to category codes\n",
    "combined_nona[\"cruise_codes\"] = combined_nona[\"cruise\"].astype(\"category\").cat.codes\n",
    "\n",
    "ax.scatter(combined_nona[\"longitude\"], combined_nona[\"latitude\"], c=combined_nona[\"cruise_codes\"],\n",
    "           transform=ccrs.PlateCarree(), s=5)\n",
    "\n",
    "#ax.scatter(full_df[\"longitude\"], full_df[\"latitude\"], c = full_df[\"cruise\"], transform=ccrs.PlateCarree(), s=5)\n",
    "\n",
    "# Add grid lines\n",
    "gl = ax.gridlines(draw_labels=True, linestyle='--')\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_right = False\n",
    "gl.bottom_labels = True  # Show x-axis labels at the bottom\n",
    "gl.left_labels = True  # Show y-axis labels on the left\n",
    "\n",
    "ax.set_extent([-180, 180, 65, 90], crs=ccrs.PlateCarree())\n",
    "ax.set_title(\"Round Map with Grid Lines and Plot Points\")\n",
    "ax.legend(loc=\"center left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up Seaborn style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Define the depth values and cruise labels\n",
    "depth_values = [combined_nona.loc[combined_nona['cruise'] == cruise, 'depth'] for cruise in combined_nona['cruise'].unique()]\n",
    "labels = combined_nona['cruise'].unique()\n",
    "\n",
    "# Plot a stacked histogram with labeled bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(depth_values, bins=50, stacked=True, label=labels, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Stacked Histogram of Depth Values by Dataset')\n",
    "plt.legend()\n",
    "\n",
    "# Remove the right and top spines\n",
    "sns.despine()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise all the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_profiles = len(HM_ds[\"longitude\"]) + len(nice_ds[\"longitude\"]) + len(mosaic_ds[\"longitude\"]) + len(alberto_ds[\"longitude\"]) + len(arctic_ds[\"longitude\"])\n",
    "print(\"total amount of profiles is: \", total_profiles)\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.NorthPolarStereo())\n",
    "\n",
    "# Add land, ocean, and borders\n",
    "ax.add_feature(cfeature.OCEAN.with_scale('50m'))\n",
    "ax.add_feature(cfeature.LAND.with_scale('50m'))\n",
    "\n",
    "ax.scatter(HM_ds[\"longitude\"], HM_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='red', s=5, label=\"HM\")\n",
    "ax.scatter(nice_ds[\"longitude\"], nice_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='blue', s=5, label=\"Nice\")\n",
    "ax.scatter(mosaic_ds[\"longitude\"], mosaic_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='green', s=5, label=\"Mosaic\")\n",
    "ax.scatter(alberto_ds[\"longitude\"], alberto_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='yellow', s=5, label=\"Alberto\")\n",
    "ax.scatter(arctic_ds[\"longitude\"], arctic_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='purple', s=5, label=\"ArcticMix\")\n",
    "\n",
    "# Add grid lines\n",
    "gl = ax.gridlines(draw_labels=True, linestyle='--')\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_right = False\n",
    "gl.bottom_labels = True  # Show x-axis labels at the bottom\n",
    "gl.left_labels = True  # Show y-axis labels on the left\n",
    "\n",
    "ax.set_extent([-180, 180, 65, 90], crs=ccrs.PlateCarree())\n",
    "ax.set_title(\"Round Map with Grid Lines and Plot Points\")\n",
    "ax.legend(loc=\"center left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_profiles = len(HM_ds[\"longitude\"]) + len(nice_ds[\"longitude\"]) + len(mosaic_ds[\"longitude\"]) + len(alberto_ds[\"longitude\"]) + len(arctic_ds[\"longitude\"])\n",
    "print(\"total amount of profiles is: \", total_profiles)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.scatter(HM_ds[\"longitude\"], HM_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='red', s=5, label = \"HM\")\n",
    "ax.scatter(nice_ds[\"longitude\"], nice_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='blue', s=5, label = \"Nice\")\n",
    "ax.scatter(mosaic_ds[\"longitude\"], mosaic_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='green', s=5, label = \"Mosaic\")\n",
    "ax.scatter(alberto_ds[\"longitude\"], alberto_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='yellow', s=5, label = \"Alberto\")\n",
    "ax.scatter(arctic_ds[\"longitude\"], arctic_ds[\"latitude\"], transform=ccrs.PlateCarree(), color='purple', s=5, label = \"ArcticMix\")\n",
    "\n",
    "# Add land, ocean, and borders\n",
    "ax.add_feature(cfeature.LAND.with_scale('50m'))\n",
    "ax.add_feature(cfeature.OCEAN.with_scale('50m'))\n",
    "\n",
    "#ax.set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())\n",
    "ax.set_title(\"Map with Land Boundaries and Plot Points\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the depth variable from each dataset\n",
    "depth1 = arctic_ds['depth'].sel()\n",
    "depth2 = alberto_ds['depth'].sel()\n",
    "depth3 = nice_ds['depth'].sel()\n",
    "depth4 = mosaic_ds['depth'].sel()\n",
    "depth5 = HM_ds['depth'].sel()\n",
    "\n",
    "# Concatenate the depth values into a single array\n",
    "depth_values = np.concatenate([depth1.values, depth2.values, depth3.values, depth4.values, depth5.values])\n",
    "\n",
    "# Plot a histogram of the combined depth values\n",
    "plt.hist(depth_values, bins=50)\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Depth Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Select the depth variable from each dataset\n",
    "depth1 = arctic_ds['depth'].sel()\n",
    "depth2 = alberto_ds['depth'].sel()\n",
    "depth3 = nice_ds['depth'].sel()\n",
    "depth4 = mosaic_ds['depth'].sel()\n",
    "depth5 = HM_ds['depth'].sel()\n",
    "\n",
    "# Combine the depth values into a single array\n",
    "depth_values = np.concatenate([depth1.values, depth2.values, depth3.values, depth4.values, depth5.values])\n",
    "\n",
    "# Create a list of labels for each dataset\n",
    "labels = ['Arctic', 'Alberto', 'Nice', 'Mosaic', 'HM']\n",
    "\n",
    "# Set up Seaborn style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Plot a stacked histogram with labeled bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist([depth1, depth2, depth3, depth4, depth5], bins=50, stacked=True, label=labels, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Stacked Histogram of Depth Values by Dataset')\n",
    "plt.legend()\n",
    "\n",
    "# Remove the right and top spines\n",
    "sns.despine()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From now on, pretend like nice_ds is the combined_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate features like in Mashayek et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_kappa = calc_N2_kappa(nice_ds)\n",
    "nice_kappa.kappa.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e-7, vmax = 1e-1),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_kappa.eps.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e-10, vmax = 1e-4),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HM_kappa = HM_ds.squeeze()\n",
    "HM_kappa = HM_kappa.transpose('depth', 'profile')\n",
    "HM = calc_N2_kappa(HM_kappa)\n",
    "HM.kappa.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e-7, vmax = 1e-4),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HM.eps.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e-11, vmax = 1e-7),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberto_kappa = calc_N2_kappa(alberto_ds)\n",
    "alberto_kappa.kappa.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e-5, vmax = 1e1),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberto_kappa.eps.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e-7, vmax = 1e1),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_ds = arctic_ds.squeeze()\n",
    "arctic = calc_N2_kappa_func(arctic_ds)\n",
    "arctic.kappa.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e-7, vmax = 1e-1),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = calc_N2_kappa(mosaic_ds)\n",
    "mosaic.kappa.plot(y = 'depth',norm = colors.LogNorm(vmin = 1e4, vmax = 1e7),cmap='viridis',figsize=(20,5))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic.eps.plot(y = 'depth', cmap='viridis',figsize=(20,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hab_func(data):\n",
    "    bathy_interp = bathy_ds.interp_like(data, method='nearest')\n",
    "    n_depths = data.profile.shape[0]\n",
    "    depth = np.zeros(n_depths)\n",
    "\n",
    "    for i in tqdm(range(n_depths)):\n",
    "        microlon = data.longitude[i].values.flatten()\n",
    "        microlat = data.latitude[i].values.flatten()\n",
    "        depth[i] = bathy_interp.elevation.sel(lon=microlon,lat=microlat, method='nearest')\n",
    "    data['bathymetry'] = data.profile.copy(data=depth)\n",
    "    \n",
    "    data[\"hab\"] = data.bathymetry + abs(data.depth)\n",
    "    data[\"hab\"] = data[\"hab\"].where(data[\"hab\"] <= 0, 0)  # Set positive values to zero\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberto_hab = calc_hab_func(alberto_ds)\n",
    "alberto_hab.bathymetry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_hab = calc_hab_func(mosaic_ds)\n",
    "mosaic_hab.bathymetry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_hab = calc_hab_func(nice_ds)\n",
    "nice_hab.bathymetry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HM_hab = calc_hab_func(HM_ds)\n",
    "HM_hab.bathymetry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctic_calchab_func(data):\n",
    "    # group data by the 'profile' dimension\n",
    "    profile_groups = data.groupby('profile')\n",
    "\n",
    "    bathy_interp = bathy_ds.interp_like(data, method='nearest')\n",
    "    n_profiles = len(data.depth)\n",
    "\n",
    "    profile = np.zeros(len(profile_groups))\n",
    "\n",
    "    # loop over each group\n",
    "    for i, (_, profile_data) in tqdm(enumerate(profile_groups)):\n",
    "        microlat = profile_data.latitude.values.flatten()[0]\n",
    "        microlon = profile_data.longitude.values.flatten()[0]\n",
    "        profile[i] = bathy_interp.elevation.sel(lon=microlon, lat=microlat, method='nearest').values\n",
    "\n",
    "    # Create a DataArray for bathymetry with the 'profile' dimension\n",
    "    profile_arr = xr.DataArray(profile, coords=[range(len(profile_groups))], dims=['profile'])\n",
    "    data['bathymetry'] = profile_arr\n",
    "    data[\"hab\"] = data.bathymetry + abs(data.depth)\n",
    "    data[\"hab\"] = data[\"hab\"].where(data[\"hab\"] <= 0, 0)  # Set positive values to zero\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_hab = arctic_calchab_func(arctic_ds)\n",
    "arctic_hab.bathymetry.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spare functions, from processing_func.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coords(data):\n",
    "    if \"latitude\" not in data.coords:\n",
    "        data = data.set_coords(\"latitude\")\n",
    "    if \"longitude\" not in data.coords:\n",
    "        data = data.set_coords(\"longitude\")\n",
    "    if \"time\" not in data.coords:\n",
    "        data = data.set_coords(\"time\")\n",
    "    if \"depth\" not in data.coords:\n",
    "        data[\"depth\"] = data.depth\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_derivative(dataset):\n",
    "  dataset[\"dTdz\"] = dataset.T.differentiate('depth')\n",
    "  dataset['dSdz'] = dataset.S.differentiate('depth')\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_pmid(dataset, variable):\n",
    "    original_depth = dataset.depth  # Assuming you have access to the original depth values\n",
    "    depth_old = np.arange(variable.shape[0])  # Depth values of the variable\n",
    "\n",
    "    # Extend the depth array to include the boundary value\n",
    "    depth_new = np.append(depth_old, original_depth[-1])\n",
    "\n",
    "    # Extend the variable array to include the boundary value\n",
    "    # new_array = np.column_stack((variable, variable[:, -1]))\n",
    "    new_array = np.vstack((variable, variable[-1, :]))\n",
    "\n",
    "    # Create a 1D interpolation function along the depth dimension\n",
    "    interp_func = interp1d(depth_new, new_array, axis=0, kind='linear')\n",
    "    # Interpolate the variable to match the original depth dimension\n",
    "    var_interp = interp_func(original_depth)\n",
    "\n",
    "    # Create a DataArray with explicit dimension names\n",
    "    var_dataarray = xr.DataArray(var_interp, dims=('depth', 'profile'))\n",
    "    return var_dataarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_pmid_arctic(dataset, variable):\n",
    "    original_depth = dataset.depth  # Assuming you have access to the original depth values\n",
    "    depth_old = np.arange(variable.shape[0])  # Depth values of the variable\n",
    "\n",
    "    # Extend the depth array to include the boundary value\n",
    "    depth_new = np.append(depth_old, original_depth[-1])\n",
    "\n",
    "    # Extend the variable array to include the boundary value\n",
    "    # new_array = np.column_stack((variable, variable[:, -1]))\n",
    "    new_array = np.vstack((variable, variable[-1, :]))\n",
    "\n",
    "    # Create a 1D interpolation function along the depth dimension\n",
    "    interp_func = interp1d(depth_new, new_array, axis=0, kind='linear')\n",
    "    # Interpolate the variable to match the original depth dimension\n",
    "    var_interp = interp_func(original_depth)\n",
    "\n",
    "    # Create a DataArray with explicit dimension names\n",
    "    var_dataarray = xr.DataArray(var_interp, dims=('depth', 'profile'))\n",
    "    return var_dataarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_N2_kappa_func(dataset, arctic=\"false\"):\n",
    "    \"\"\"N2 and kappa both independent of epsilon\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : dataset\n",
    "        Microstructure dataset, where insitu temperature is named as \"T\" in\n",
    "        degrees Celcius.\n",
    "        Salinity is named as \"S\" in .., and depth is called \"depth\" in meters.\n",
    "    \"\"\"\n",
    "    S = dataset.S\n",
    "    P = dataset.P\n",
    "    lon = dataset.longitude.squeeze()\n",
    "    lat = dataset.latitude.squeeze()\n",
    "    T = dataset.T\n",
    "    eps = dataset.eps\n",
    "    z = dataset.depth\n",
    "    # Add a dummy axis to pressure (P) to match the shape of other variables\n",
    "    if T.shape != P.shape:\n",
    "        # Project the lower-dimensional variable onto the target dimension\n",
    "        P = np.expand_dims(P, axis=1)\n",
    "\n",
    "    # convert measured T to potential T, which is seen as temperature now\n",
    "    # potential temperature is the temperature a water parcel would have \n",
    "    # if it were brought to the surface adiabatically (no pressure effects)\n",
    "    dataset = dataset.rename({\"T\": \"insituT\"})\n",
    "    dataset[\"T\"] = gsw.conversions.pt_from_t(S, T, P, p_ref=0)\n",
    "    dataset['rho'] = gsw.rho(S, T, P)\n",
    "    dataset[\"SA\"] = gsw.SA_from_SP(S, P, lon, lat)\n",
    "    dataset[\"CT\"] = gsw.CT_from_t(dataset.SA, T, P)\n",
    "\n",
    "    # calculate the turner angle\n",
    "    # The values of Turner Angle Tu and density ratio Rrho are calculated\n",
    "    # at mid-point pressures, p_mid.\n",
    "    # https://teos-10.org/pubs/gsw/html/gsw_Turner_Rsubrho.html\n",
    "    [Tu, Rsubrho, p_mid] = gsw.Turner_Rsubrho(dataset.SA, dataset.CT, P)\n",
    "\n",
    "    dataset[\"Tu\"] = interpolate_pmid(dataset, Tu)\n",
    "    dataset[\"Rsubrho\"] = interpolate_pmid(dataset, Rsubrho)\n",
    "\n",
    "    # Calculate N^2 using gsw_Nsquared\n",
    "    # https://teos-10.org/pubs/gsw/html/gsw_Nsquared.html\n",
    "    [N2, p_mid] = gsw.Nsquared(SA=dataset[\"SA\"], CT=dataset[\"CT\"], p=P,\n",
    "                               lat=dataset[\"latitude\"])\n",
    "    dataset[\"N2\"] = interpolate_pmid(dataset, N2)\n",
    "\n",
    "    # calculate kappa like in Mashayek et al, 2022\n",
    "    # assume chi is 0.2 in standard turbulence regime\n",
    "    dataset['kappa'] = 0.2*dataset.eps/dataset.N2\n",
    "    # assume mixing efficiency of 1 in double diffusion regime\n",
    "    dataset[\"kappa_AT\"] = dataset.eps/dataset.N2\n",
    "\n",
    "    dataset[\"log_N2\"] = np.log10(dataset.N2)\n",
    "    dataset[\"log_kappa\"] = np.log10(dataset.kappa)\n",
    "\n",
    "    dataset = TS_derivative(dataset)\n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('arcticT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dbe666d3ff0a912f7bf0b5ebf79f120d34d32fb27f825149a704cbc210e489f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
